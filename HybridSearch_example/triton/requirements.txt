# transformers==4.44.* # <--- for florence2base
# huggingface_hub[cli]==0.24.* # <--- for florence2base
transformers==4.51.3 # <--- qwen2.5 needs this version
autoawq>=0.1.8 # <--- for quantization used on qwen2.5
accelerate>=0.26.0 # <--- for loading weights on gpu
numpy<2 # <--- numpy 2.0 is not compatible with transformers
huggingface_hub[cli]==0.30.* # <--- upgraded for transformers==4.51.3
Pillow==10.4.*
timm==1.0.*
einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above 