# transformers==4.44.* # <--- for florence2base
transformers==4.49.* # <--- qwen2.5 needs this version or newer
huggingface_hub[cli]==0.24.*
Pillow==10.4.*
timm==1.0.*
einops==0.8.*
# packaging==24.2.* #enable for flash attention, flash attention must have have CUDA 11.7 and above 